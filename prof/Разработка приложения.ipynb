{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50398ae3",
   "metadata": {},
   "source": [
    "# Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ea5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "st.title(\"ML Prediction App\")\n",
    "\n",
    "# –§–æ—Ä–º–∞ –¥–ª—è –≤–≤–æ–¥–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "with st.form(\"prediction_form\"):\n",
    "    age = st.number_input(\"Age\", min_value=0, max_value=120, value=30)\n",
    "    income = st.number_input(\"Income\", min_value=0, value=50000)\n",
    "    category = st.selectbox(\"Category\", [\"A\", \"B\", \"C\"])\n",
    "    feature1 = st.slider(\"Feature 1\", 0.0, 1.0, 0.5)\n",
    "    feature2 = st.slider(\"Feature 2\", 0.0, 100.0, 50.0)\n",
    "    \n",
    "    submitted = st.form_submit_button(\"Predict\")\n",
    "\n",
    "if submitted:\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è API\n",
    "    data = {\n",
    "        \"age\": age,\n",
    "        \"income\": income,\n",
    "        \"category\": category,\n",
    "        \"feature1\": feature1,\n",
    "        \"feature2\": feature2\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ API\n",
    "        response = requests.post(\"http://localhost:8000/predict\", json=data)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            st.success(f\"Prediction: {result['prediction']}\")\n",
    "            st.info(f\"Probability: {result['probability']:.2%}\")\n",
    "            \n",
    "            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "            col1, col2 = st.columns(2)\n",
    "            with col1:\n",
    "                st.metric(\"Prediction\", result['prediction'])\n",
    "            with col2:\n",
    "                st.metric(\"Confidence\", f\"{result['probability']:.1%}\")\n",
    "        else:\n",
    "            st.error(f\"API Error: {response.text}\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Connection error: {e}\")\n",
    "\n",
    "# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å\n",
    "st.sidebar.header(\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\")\n",
    "st.sidebar.info(\"–≠—Ç–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç ML –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\")\n",
    "st.sidebar.code(\"API: http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1f35b",
   "metadata": {},
   "source": [
    "pip install streamlit requests\n",
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c6ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_example.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "data = pd.DataFrame({\n",
    "    'age': np.random.randint(20, 70, 100),\n",
    "    'income': np.random.randint(20000, 150000, 100),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100),\n",
    "    'target': np.random.randint(0, 2, 100)\n",
    "})\n",
    "\n",
    "# 2. –û–±—É—á–µ–Ω–∏–µ\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è\n",
    "X_train_encoded = encoder.fit_transform(X_train[['category']])\n",
    "X_test_encoded = encoder.transform(X_test[['category']])\n",
    "\n",
    "X_train_final = np.hstack([X_train[['age', 'income']].values, X_train_encoded])\n",
    "X_test_final = np.hstack([X_test[['age', 'income']].values, X_test_encoded])\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "# –ú–æ–¥–µ–ª—å\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(encoder, 'encoder.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n",
    "print(\"   model.pkl - ML –º–æ–¥–µ–ª—å\")\n",
    "print(\"   encoder.pkl - OneHotEncoder\")\n",
    "print(\"   scaler.pkl - StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636b68b",
   "metadata": {},
   "source": [
    "–ö—Ä–∞—Ç–∫–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞:\n",
    "–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–∏ –º–æ–¥–µ–ª–∏ (train_and_save.py)\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏ API (uvicorn main:app --reload)\n",
    "\n",
    "–ó–∞–ø—É—Å—Ç–∏ Streamlit (streamlit run app.py)\n",
    "\n",
    "–ü—Ä–æ–≤–µ—Ä—å —Ä–∞–±–æ—Ç—É —á–µ—Ä–µ–∑ http://localhost:8501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a621453",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:\n",
    "–í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π, —á—Ç–æ API –∑–∞–ø—É—â–µ–Ω –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º Streamlit\n",
    "\n",
    "–°–æ—Ö—Ä–∞–Ω—è–π –í–°–ï —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã (scaler, encoder)\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π joblib –¥–ª—è sklearn –º–æ–¥–µ–ª–µ–π, model.save() –¥–ª—è Keras\n",
    "\n",
    "–î–æ–±–∞–≤—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –≤ API\n",
    "\n",
    "Streamlit –æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π - –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dbc353",
   "metadata": {},
   "source": [
    "fastapi==0.104.1\n",
    "uvicorn==0.24.0\n",
    "streamlit==1.28.0\n",
    "requests==2.31.0\n",
    "pandas==2.1.3\n",
    "scikit-learn==1.3.2\n",
    "joblib==1.3.2\n",
    "numpy==1.24.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff17c4",
   "metadata": {},
   "source": [
    "# –í—Å–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞ \"–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—ã\"\n",
    "\n",
    "# ==================== –ë–ê–ó–û–í–´–ï ====================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–• ====================\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, OneHotEncoder, \n",
    "    LabelEncoder, OrdinalEncoder\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, cross_validate,\n",
    "    GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# ==================== –ú–û–î–ï–õ–ò ML ====================\n",
    "# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier,\n",
    "    VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# –†–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, GradientBoostingRegressor\n",
    ")\n",
    "\n",
    "# –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ==================== –ú–ï–¢–†–ò–ö–ò ====================\n",
    "from sklearn.metrics import (\n",
    "    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    \n",
    "    # –†–µ–≥—Ä–µ—Å—Å–∏—è\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    \n",
    "    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è\n",
    "    silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    ")\n",
    "\n",
    "# ==================== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ====================\n",
    "from scipy import stats\n",
    "from scipy.stats import (\n",
    "    shapiro, kstest, mannwhitneyu, ttest_ind,\n",
    "    f_oneway, kruskal, chi2_contingency\n",
    ")\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# ==================== –†–ê–ó–ú–ï–†–ù–û–°–¢–¨ ====================\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# ==================== –¢–ï–ö–°–¢ ====================\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import pymorphy2\n",
    "\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from wordcloud import WordCloud\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# ==================== –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø ====================\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "\n",
    "# ==================== –í–†–ï–ú–ï–ù–ù–´–ï –†–Ø–î–´ ====================\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# ==================== –ë–£–°–¢–ò–ù–ì–ò (–í–ê–ñ–ù–û!) ====================\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "\n",
    "# ==================== –ù–ï–ô–†–û–°–ï–¢–ò ====================\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ==================== API –ò –î–ï–ü–õ–û–ô ====================\n",
    "import joblib\n",
    "import pickle\n",
    "import json\n",
    "from fastapi import FastAPI\n",
    "import streamlit as st\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387f947",
   "metadata": {},
   "source": [
    "# –ü–æ–ª–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
    "pip install pandas numpy matplotlib seaborn scikit-learn scipy statsmodels \\\n",
    "            xgboost lightgbm catboost tensorflow \\\n",
    "            nltk gensim wordcloud pymorphy2 \\\n",
    "            opencv-python Pillow \\\n",
    "            fastapi uvicorn streamlit requests joblib\n",
    "\n",
    "# –î–ª—è NLP –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ\n",
    "python -m nltk.downloader punkt stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862083d",
   "metadata": {},
   "source": [
    "project/\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ train.py          # –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
    "‚îú‚îÄ‚îÄ api.py            # FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\n",
    "‚îú‚îÄ‚îÄ app.py            # Streamlit –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\n",
    "‚îú‚îÄ‚îÄ models/           # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ model.pkl\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ encoder.pkl\n",
    "‚îú‚îÄ‚îÄ notebooks/        # —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\n",
    "‚îî‚îÄ‚îÄ data/            # –¥–∞–Ω–Ω—ã–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è FastAPI (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è)\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"ML Prediction API\",\n",
    "    description=\"API –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –ø–æ–º–æ—â—å—é ML –º–æ–¥–µ–ª–∏\",\n",
    "    version=\"1.0.0\",\n",
    "    docs_url=\"/docs\",      # Swagger UI\n",
    "    redoc_url=\"/redoc\"     # ReDoc\n",
    ")\n",
    "\n",
    "class InputData(BaseModel):\n",
    "    \"\"\"–ú–æ–¥–µ–ª—å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\"\"\"\n",
    "    age: float = 30.0\n",
    "    income: float = 50000.0\n",
    "    category: str = \"A\"\n",
    "    \n",
    "    class Config:\n",
    "        schema_extra = {\n",
    "            \"example\": {\n",
    "                \"age\": 35,\n",
    "                \"income\": 75000,\n",
    "                \"category\": \"B\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "class Prediction(BaseModel):\n",
    "    \"\"\"–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º\"\"\"\n",
    "    prediction: float\n",
    "    probability: float\n",
    "    status: str = \"success\"\n",
    "\n",
    "@app.get(\"/\", tags=[\"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\"])\n",
    "def read_root():\n",
    "    \"\"\"–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç\"\"\"\n",
    "    return {\"message\": \"ML API —Ä–∞–±–æ—Ç–∞–µ—Ç\"}\n",
    "\n",
    "@app.post(\"/predict\", response_model=Prediction, tags=[\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\"])\n",
    "async def predict(data: InputData):\n",
    "    \"\"\"\n",
    "    –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ –≤—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º\n",
    "    \n",
    "    - **age**: –≤–æ–∑—Ä–∞—Å—Ç (–ª–µ—Ç)\n",
    "    - **income**: –¥–æ—Ö–æ–¥ (—Ä—É–±)\n",
    "    - **category**: –∫–∞—Ç–µ–≥–æ—Ä–∏—è (A/B/C)\n",
    "    \"\"\"\n",
    "    # –í–∞—à –∫–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    return {\n",
    "        \"prediction\": 1.0,\n",
    "        \"probability\": 0.85,\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\", tags=[\"–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥\"])\n",
    "def health_check():\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "# –î–ª—è –∑–∞–ø—É—Å–∫–∞: uvicorn main:app --reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a713d7",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61300ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_openapi.py\n",
    "import json\n",
    "from main import app  # –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è OpenAPI —Å—Ö–µ–º—ã\n",
    "openapi_schema = app.openapi()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON\n",
    "with open(\"openapi.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(openapi_schema, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ openapi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9295aa",
   "metadata": {},
   "source": [
    "Streamlit –¥–æ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e60119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(\n",
    "    page_title=\"ML Prediction App\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\"\n",
    ")\n",
    "\n",
    "# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π\n",
    "with st.sidebar:\n",
    "    st.title(\"üìö –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\")\n",
    "    st.markdown(\"\"\"\n",
    "    ### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:\n",
    "    1. –í–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º—É\n",
    "    2. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É Predict\n",
    "    3. –ü–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "    \n",
    "    ### –ü–æ–ª—è:\n",
    "    - **Age**: –≤–æ–∑—Ä–∞—Å—Ç (20-70 –ª–µ—Ç)\n",
    "    - **Income**: –¥–æ—Ö–æ–¥ –≤ —Ä—É–±–ª—è—Ö\n",
    "    - **Category**: –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∫–ª–∏–µ–Ω—Ç–∞\n",
    "    \n",
    "    ### API:\n",
    "    - **URL**: http://localhost:8000\n",
    "    - **–≠–Ω–¥–ø–æ–∏–Ω—Ç**: /predict\n",
    "    - **–ú–µ—Ç–æ–¥**: POST\n",
    "    \"\"\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞\n",
    "    with st.expander(\"–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞ API\"):\n",
    "        st.code(\"\"\"\n",
    "{\n",
    "  \"age\": 35,\n",
    "  \"income\": 75000,\n",
    "  \"category\": \"B\"\n",
    "}\n",
    "        \"\"\", language=\"json\")\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å\n",
    "st.title(\"ü§ñ ML Prediction Application\")\n",
    "st.markdown(\"–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ML –º–æ–¥–µ–ª–µ–π\")\n",
    "\n",
    "# –§–æ—Ä–º–∞ –≤–≤–æ–¥–∞\n",
    "with st.form(\"input_form\"):\n",
    "    col1, col2, col3 = st.columns(3)\n",
    "    \n",
    "    with col1:\n",
    "        age = st.number_input(\"–í–æ–∑—Ä–∞—Å—Ç\", 18, 100, 30)\n",
    "    \n",
    "    with col2:\n",
    "        income = st.number_input(\"–î–æ—Ö–æ–¥\", 0, 1000000, 50000)\n",
    "    \n",
    "    with col3:\n",
    "        category = st.selectbox(\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è\", [\"A\", \"B\", \"C\"])\n",
    "    \n",
    "    submitted = st.form_submit_button(\"üöÄ –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\")\n",
    "\n",
    "if submitted:\n",
    "    # –í–∞—à –∫–æ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "    st.success(\"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ–ª—É—á–µ–Ω–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae5846",
   "metadata": {},
   "source": [
    "4. README.md –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞\n",
    "markdown\n",
    "# ML Prediction API\n",
    "\n",
    "## –û–ø–∏—Å–∞–Ω–∏–µ\n",
    "API –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "–ó–∞–ø—É—Å–∫\n",
    "bash\n",
    "# –ó–∞–ø—É—Å–∫ API\n",
    "uvicorn main:app --reload --host 0.0.0.0 --port 8000\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ Streamlit\n",
    "streamlit run app.py\n",
    "API –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã\n",
    "GET /\n",
    "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã API\n",
    "\n",
    "–û—Ç–≤–µ—Ç:\n",
    "\n",
    "json\n",
    "{\"message\": \"ML API —Ä–∞–±–æ—Ç–∞–µ—Ç\"}\n",
    "POST /predict\n",
    "–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
    "\n",
    "–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:\n",
    "\n",
    "json\n",
    "{\n",
    "  \"age\": 35,\n",
    "  \"income\": 75000,\n",
    "  \"category\": \"B\"\n",
    "}\n",
    "–û—Ç–≤–µ—Ç:\n",
    "\n",
    "json\n",
    "{\n",
    "  \"prediction\": 1.0,\n",
    "  \"probability\": 0.85,\n",
    "  \"status\": \"success\"\n",
    "}\n",
    "GET /health\n",
    "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è\n",
    "\n",
    "–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "Python\n",
    "python\n",
    "import requests\n",
    "\n",
    "data = {\"age\": 35, \"income\": 75000, \"category\": \"B\"}\n",
    "response = requests.post(\"http://localhost:8000/predict\", json=data)\n",
    "print(response.json())\n",
    "cURL\n",
    "bash\n",
    "curl -X POST http://localhost:8000/predict \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"age\": 35, \"income\": 75000, \"category\": \"B\"}'\n",
    "–ú–æ–¥–µ–ª–∏\n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º: RandomForestClassifier\n",
    "\n",
    "–¢–æ—á–Ω–æ—Å—Ç—å: 0.89\n",
    "\n",
    "–î–∞—Ç–∞ –æ–±—É—á–µ–Ω–∏—è: 2024-01-15\n",
    "\n",
    "–ê–≤—Ç–æ—Ä—ã\n",
    "–í–∞—à–∞ –∫–æ–º–∞–Ω–¥–∞\n",
    "\n",
    "text\n",
    "\n",
    "---\n",
    "\n",
    "# **5. requirements.txt —Å –≤–µ—Ä—Å–∏—è–º–∏**\n",
    "\n",
    "```txt\n",
    "# API\n",
    "fastapi==0.104.1\n",
    "uvicorn[standard]==0.24.0\n",
    "pydantic==2.5.0\n",
    "\n",
    "# –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\n",
    "streamlit==1.28.1\n",
    "\n",
    "# ML\n",
    "scikit-learn==1.3.2\n",
    "joblib==1.3.2\n",
    "pandas==2.1.4\n",
    "numpy==1.24.3\n",
    "\n",
    "# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ\n",
    "python-multipart==0.0.6  # –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤\n",
    "6. Dockerfile —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π\n",
    "dockerfile\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º –∫–æ–¥\n",
    "COPY . .\n",
    "\n",
    "# –ö–æ–ø–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é\n",
    "COPY README.md .\n",
    "COPY openapi.json .\n",
    "\n",
    "# –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ä—Ç—ã\n",
    "EXPOSE 8000 8501\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ API\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "–ë—ã—Å—Ç—Ä—ã–π —á–µ–∫ –¥–ª—è —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞:\n",
    "–í FastAPI: –î–æ–±–∞–≤—å –æ–ø–∏—Å–∞–Ω–∏–µ –≤ –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã\n",
    "\n",
    "–í Streamlit: –ò—Å–ø–æ–ª—å–∑—É–π st.sidebar –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏\n",
    "\n",
    "–°–æ–∑–¥–∞–π README.md —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∑–∞–ø—Ä–æ—Å–æ–≤\n",
    "\n",
    "–°–æ—Ö—Ä–∞–Ω–∏ OpenAPI —Å—Ö–µ–º—É: json.dump(app.openapi(), ...)\n",
    "\n",
    "–î–æ–±–∞–≤—å –ø—Ä–∏–º–µ—Ä—ã curl –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é\n",
    "\n",
    "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:\n",
    "\n",
    "python\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: InputData):\n",
    "    \"\"\"\n",
    "    Predict endpoint\n",
    "    \n",
    "    Example:\n",
    "    ```json\n",
    "    {\"age\": 30, \"income\": 50000}\n",
    "    ```\n",
    "    \"\"\"\n",
    "    return {\"prediction\": 1.0}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
