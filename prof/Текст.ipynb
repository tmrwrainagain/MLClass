{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4d3a1d",
   "metadata": {},
   "source": [
    "# Нормализация "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040337d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация \n",
    "import re\n",
    "import pymorphy2\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Инициализация\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Текст\n",
    "text = \"В 2024 году я купил 2 новых iPhone за 100000 рублей!!!\"\n",
    "\n",
    "# 1. Убрать числа и спецсимволы\n",
    "clean_text = re.sub(r'[^а-яА-Яa-zA-Z\\s]', ' ', text)  # оставляем только буквы и пробелы\n",
    "\n",
    "# 2. Перевести в нижний регистр\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "# 3. Разбить на токены\n",
    "tokens = word_tokenize(clean_text, language='russian')\n",
    "\n",
    "# 4. Нормальные формы\n",
    "normalized_tokens = [morph.parse(token)[0].normal_form for token in tokens if token.strip()]\n",
    "\n",
    "print(f\"Исходный текст: {text}\")\n",
    "print(f\"Токены: {tokens}\")\n",
    "print(f\"Нормальные формы: {normalized_tokens}\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Убрать всё кроме букв и пробелов\n",
    "    text = re.sub(r'[^а-яА-Яa-zA-Z\\s]', ' ', text)\n",
    "    # В нижний регистр\n",
    "    text = text.lower()\n",
    "    # Разбить на слова\n",
    "    words = text.split()\n",
    "    # Нормальные формы (если pymorphy2 доступен)\n",
    "    if 'pymorphy2' in globals():\n",
    "        return [morph.parse(w)[0].normal_form for w in words]\n",
    "    return words\n",
    "\n",
    "normalized = normalize_text(\"Привет, как дела? 2024 год!\")\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "text = \"Пример текста 2024!!!\"\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# Все в одной строке\n",
    "normalized = [morph.parse(word)[0].normal_form for word in \n",
    "              re.sub(r'[^а-яА-Яa-zA-Z\\s]', ' ', text).lower().split() if word]\n",
    "\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea3b7a",
   "metadata": {},
   "source": [
    "## Векторизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Пример текстов\n",
    "texts = [\"машинное обучение это круто\", \n",
    "         \"глубокое обучение нейронные сети\", \n",
    "         \"машинное и глубокое обучение\"]\n",
    "\n",
    "# 1. CountVectorizer (мешок слов)\n",
    "count_vectorizer = CountVectorizer(\n",
    "    min_df=1,      # минимальная частота слова\n",
    "    max_df=1.0,    # максимальная частота (1.0 = 100%)\n",
    "    max_features=1000,  # максимальное число признаков\n",
    "    ngram_range=(1, 3)  # униграммы, биграммы, триграммы\n",
    ")\n",
    "X_count = count_vectorizer.fit_transform(texts)\n",
    "print(f\"CountVectorizer: {X_count.shape}\")\n",
    "\n",
    "# 2. TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=1,\n",
    "    max_df=1.0,\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2)  # униграммы + биграммы\n",
    ")\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "print(f\"TF-IDF: {X_tfidf.shape}\")\n",
    "\n",
    "# 3. Word2Vec\n",
    "# Подготовка токенизированных текстов\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "# Обучение Word2Vec\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_texts,\n",
    "    vector_size=100,      # размер вектора\n",
    "    window=5,            # окно контекста\n",
    "    min_count=1,         # минимальная частота слова\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Вектор для всего документа (среднее векторов слов)\n",
    "def doc_vector(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "X_word2vec = np.array([doc_vector(tokens, word2vec_model) for tokens in tokenized_texts])\n",
    "print(f\"Word2Vec: {X_word2vec.shape}\")\n",
    "\n",
    "# Быстрая проверка\n",
    "print(\"\\nПараметры и результаты:\")\n",
    "print(f\"Слова CountVectorizer: {count_vectorizer.get_feature_names_out()[:5]}\")\n",
    "print(f\"Слова TF-IDF: {tfidf_vectorizer.get_feature_names_out()[:5]}\")\n",
    "print(f\"Слова Word2Vec: {list(word2vec_model.wv.index_to_key)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e06c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF с настройками\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=2,           # игнорировать слова встречающиеся <2 раз\n",
    "    max_df=0.9,         # игнорировать слова встречающиеся >90% документов\n",
    "    max_features=5000,  # максимум 5000 признаков\n",
    "    ngram_range=(1, 2)  # слова и пары слов\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(texts)\n",
    "print(f\"Размерность: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Важные параметры:\n",
    "# min_df=2 - убрать редкие слова (шум)\n",
    "# max_df=0.9 - убрать слишком частые слова (стоп-слова)\n",
    "# max_features=5000 - ограничить размерность\n",
    "# ngram_range=(1, 3) - учитывать словосочетания\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=2,\n",
    "    max_df=0.9, \n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='russian'  # удалить стоп-слова\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0472f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Быстрый Word2Vec (если текстов много)\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Токенизированные тексты\n",
    "tokenized = [text.split() for text in texts]\n",
    "\n",
    "# Обучение\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Вектор документа = среднее векторов слов\n",
    "doc_vectors = [np.mean([model.wv[word] for word in doc if word in model.wv], axis=0) \n",
    "               for doc in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer - частота слов\n",
    "# TF-IDF - важность слов в документах  \n",
    "# Word2Vec - семантические векторы\n",
    "\n",
    "# Выбор:\n",
    "# 1. Для классификации/кластеризации → TF-IDF\n",
    "# 2. Для семантического поиска → Word2Vec\n",
    "# 3. Для быстрого базлайна → CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.9, ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(texts)\n",
    "print(f\"Признаков: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5723de",
   "metadata": {},
   "source": [
    "## Семантика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44486cba",
   "metadata": {},
   "source": [
    "весь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135432c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Пример текстов\n",
    "texts = [\"машинное обучение и глубокое обучение\", \n",
    "         \"нейронные сети и искусственный интеллект\",\n",
    "         \"анализ данных и машинное обучение\"]\n",
    "\n",
    "# 1. Популярные словосочетания (n-граммы)\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3), max_features=50)  # биграммы и триграммы\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Считаем частоты\n",
    "ngram_counts = X.sum(axis=0).A1\n",
    "ngram_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Топ-10 популярных словосочетаний\n",
    "top_ngrams = sorted(zip(ngram_names, ngram_counts), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Топ-10 популярных словосочетаний:\")\n",
    "for ngram, count in top_ngrams:\n",
    "    print(f\"  {ngram}: {count}\")\n",
    "\n",
    "# 2. Тематическое моделирование (LDA)\n",
    "# Сначала создаем матрицу \"термины-документы\"\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "tf = tf_vectorizer.fit_transform(texts)\n",
    "\n",
    "# LDA модель\n",
    "n_topics = 3  # количество тем\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics,\n",
    "    max_iter=10,\n",
    "    learning_method='online',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "lda.fit(tf)\n",
    "\n",
    "# Вывод тем\n",
    "print(f\"\\nТемы (LDA, n_topics={n_topics}):\")\n",
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"\\nТема #{topic_idx}:\")\n",
    "    # Топ-10 слов для темы\n",
    "    top_words_idx = topic.argsort()[:-11:-1]\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"  {' '.join(top_words)}\")\n",
    "\n",
    "# Распределение тем по документам\n",
    "doc_topic_dist = lda.transform(tf)\n",
    "print(f\"\\nРаспределение тем по документам:\")\n",
    "for i, dist in enumerate(doc_topic_dist):\n",
    "    print(f\"Документ {i}: {dist.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602a641",
   "metadata": {},
   "source": [
    "Кратко"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434b92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# LDA быстро\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Топ-слова для каждой темы\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Тема {i}:\", ' '.join([vectorizer.get_feature_names_out()[j] \n",
    "                                  for j in topic.argsort()[-10:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e163cd6",
   "metadata": {},
   "source": [
    "Популярные словосочетания быстро:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Биграммы\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "X_bigram = bigram_vectorizer.fit_transform(texts)\n",
    "\n",
    "# Топ-5 биграмм\n",
    "bigram_counts = X_bigram.sum(axis=0).A1\n",
    "top_bigrams = sorted(zip(bigram_vectorizer.get_feature_names_out(), bigram_counts), \n",
    "                     key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Топ биграммы:\", top_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bc4b3",
   "metadata": {},
   "source": [
    "LDA с настройками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Важные параметры LDA:\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=5,      # число тем\n",
    "    max_iter=10,         # итераций\n",
    "    learning_method='online',  # для больших данных\n",
    "    random_state=42,\n",
    "    n_jobs=-1            # параллельные вычисления\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e82c58",
   "metadata": {},
   "source": [
    "Визуализация тем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbccc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Визуализация распределения тем для документа\n",
    "doc_idx = 0\n",
    "topic_dist = lda.transform(X)[doc_idx]\n",
    "\n",
    "plt.bar(range(len(topic_dist)), topic_dist)\n",
    "plt.xlabel('Тема')\n",
    "plt.ylabel('Вероятность')\n",
    "plt.title(f'Распределение тем для документа {doc_idx}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147aa78a",
   "metadata": {},
   "source": [
    "Все в 4 строках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d621ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# LDA\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "lda = LatentDirichletAllocation(n_components=3).fit(X)\n",
    "\n",
    "# Темы\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Тема {i}:\", ' '.join(vectorizer.get_feature_names_out()[topic.argsort()[-5:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce806db4",
   "metadata": {},
   "source": [
    "n-граммы → для поиска устойчивых словосочетаний\n",
    "\n",
    "LDA → для тематического анализа\n",
    "\n",
    "max_features в CountVectorizer → ограничивает словарь\n",
    "\n",
    "n_components в LDA → подбирается экспериментально\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c625f",
   "metadata": {},
   "source": [
    "## Визуализация текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeab6bb",
   "metadata": {},
   "source": [
    "весь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ed237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Пример данных\n",
    "texts = [\"машинное обучение\", \"глубокое обучение\", \"нейронные сети\", \n",
    "         \"футбол матч гол\", \"баскетбол мяч кольцо\", \"теннис ракетка\"]\n",
    "labels = [\"ai\", \"ai\", \"ai\", \"sport\", \"sport\", \"sport\"]  # классы\n",
    "\n",
    "# 1. Векторизация текстов\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 2. Уменьшение размерности до 2D\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X.toarray())\n",
    "\n",
    "# 3. Визуализация по классам\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(labels)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_encoded, cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Подписи точек\n",
    "for i, txt in enumerate(texts[:6]):  # первые 6 для наглядности\n",
    "    plt.annotate(txt[:10], (X_2d[i, 0], X_2d[i, 1]), fontsize=9)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Визуализация текстов (PCA)')\n",
    "plt.colorbar(scatter, label='Классы')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 4. 3D визуализация\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_3d = pca_3d.fit_transform(X.toarray())\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter_3d = ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], \n",
    "                        c=y_encoded, cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title('3D визуализация текстов')\n",
    "plt.show()\n",
    "\n",
    "# 5. Облако слов для каждого класса\n",
    "all_text = ' '.join(texts)\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Общее облако слов')\n",
    "\n",
    "# Облако слов по классам\n",
    "plt.subplot(1, 2, 2)\n",
    "class_texts = {\n",
    "    'AI': ' '.join([t for t, l in zip(texts, labels) if l == 'ai']),\n",
    "    'Sport': ' '.join([t for t, l in zip(texts, labels) if l == 'sport'])\n",
    "}\n",
    "\n",
    "for idx, (cls, txt) in enumerate(class_texts.items()):\n",
    "    wc = WordCloud(width=400, height=200, background_color='white').generate(txt)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Класс: {cls}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84243d",
   "metadata": {},
   "source": [
    "кратко"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c3696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA визуализация текстов\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Векторизация и PCA\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X_2d = PCA(n_components=2).fit_transform(X.toarray())\n",
    "\n",
    "# График\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels)\n",
    "plt.show()\n",
    "\n",
    "# Облако слов\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud().generate(' '.join(texts))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4670ba",
   "metadata": {},
   "source": [
    "3д"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X_3d = PCA(n_components=3).fit_transform(X.toarray())\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bca8eba",
   "metadata": {},
   "source": [
    "Облако"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c303e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Для каждого класса свое облако\n",
    "for label in set(labels):\n",
    "    class_texts = [t for t, l in zip(texts, labels) if l == label]\n",
    "    wordcloud = WordCloud().generate(' '.join(class_texts))\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.title(f'Класс: {label}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4178522",
   "metadata": {},
   "source": [
    "TSNE НО ДОЛГИЙ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# TSNE лучше сохраняет локальные связи\n",
    "X_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.toarray())\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, alpha=0.7)\n",
    "plt.title('TSNE визуализация текстов')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24ac0aa",
   "metadata": {},
   "source": [
    "Коротко в 5 стр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488264a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = TfidfVectorizer().fit_transform(texts)\n",
    "X_2d = PCA(n_components=2).fit_transform(X.toarray())\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], c=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c231a",
   "metadata": {},
   "source": [
    "Правила:\n",
    "\n",
    "PCA → быстрая линейная визуализация\n",
    "\n",
    "TSNE → качественная нелинейная визуализация (медленнее)\n",
    "\n",
    "Цвет точек = классы\n",
    "\n",
    "Облако слов → частотность терминов\n",
    "\n",
    "Размер точек можно сделать пропорциональным длине текста\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5761043",
   "metadata": {},
   "source": [
    "Классификация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Данные\n",
    "texts = [\"машинное обучение\", \"глубокое обучение\", \"нейронные сети\", \n",
    "         \"футбол матч гол\", \"баскетбол мяч кольцо\", \"теннис ракетка\"]\n",
    "labels = [0, 0, 0, 1, 1, 1]  # 0=AI, 1=Sport\n",
    "\n",
    "# 1. Традиционные ML методы (после векторизации)\n",
    "# TF-IDF + классификаторы\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "print(\"Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# 2. Нейросети с Keras Tokenizer\n",
    "# Токенизация\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X_seq = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Паддинг до одинаковой длины\n",
    "max_len = max(len(x) for x in X_seq)\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Разделение для нейросети\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(\n",
    "    X_pad, np.array(labels), test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Простая нейросеть\n",
    "model_simple = keras.Sequential([\n",
    "    layers.Embedding(input_dim=1000, output_dim=64, input_length=max_len),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # бинарная классификация\n",
    "])\n",
    "\n",
    "model_simple.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_simple.fit(X_train_nn, y_train_nn, epochs=10, batch_size=2, verbose=0)\n",
    "\n",
    "# 4. Нейросеть с LSTM\n",
    "model_lstm = keras.Sequential([\n",
    "    layers.Embedding(input_dim=1000, output_dim=64, input_length=max_len),\n",
    "    layers.LSTM(64, return_sequences=False),  # LSTM слой\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_nn, y_train_nn, epochs=10, batch_size=2, verbose=0)\n",
    "\n",
    "# Оценка моделей\n",
    "print(\"\\nНейросети:\")\n",
    "print(\"Простая сеть - Accuracy:\", model_simple.evaluate(X_test_nn, y_test_nn, verbose=0)[1])\n",
    "print(\"LSTM сеть - Accuracy:\", model_lstm.evaluate(X_test_nn, y_test_nn, verbose=0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ce745",
   "metadata": {},
   "source": [
    "Кратко"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TF-IDF + классификаторы\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"F1: {f1_score(y_test, y_pred):.3f}\")\n",
    "\n",
    "# 2. Нейросеть с LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "# Токенизация\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X_seq = pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=50)\n",
    "\n",
    "# LSTM модель\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(1000, 64),\n",
    "    tf.keras.layers.LSTM(64),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_seq, labels, epochs=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75526269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Би-LSTM (лучше для контекста)\n",
    "model_bilstm = keras.Sequential([\n",
    "    layers.Embedding(1000, 64),\n",
    "    layers.Bidirectional(layers.LSTM(32)),  # двунаправленный LSTM\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b844d7c",
   "metadata": {},
   "source": [
    "# Что использовать:\n",
    "# 1. Если данных мало → TF-IDF + LogisticRegression/RandomForest\n",
    "# 2. Если данных много → LSTM/Bi-LSTM\n",
    "# 3. Если нужна скорость → TF-IDF + линейные модели\n",
    "# 4. Если важен контекст → LSTM с Attention\n",
    "\n",
    "# Параметры токенизатора:\n",
    "# num_words=10000 - ограничить словарь\n",
    "# max_len=100 - обрезать длинные тексты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227e61e0",
   "metadata": {},
   "source": [
    "6 стр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46862e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Традиционный ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = TfidfVectorizer().fit_transform(texts)\n",
    "model = LogisticRegression().fit(X, labels)\n",
    "print(\"Accuracy:\", model.score(X, labels))\n",
    "\n",
    "# Нейросеть\n",
    "model_lstm = keras.Sequential([layers.Embedding(1000, 64), layers.LSTM(64), layers.Dense(1, activation='sigmoid')])\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f62c7",
   "metadata": {},
   "source": [
    "многоклассовая"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb92966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для N классов\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(1000, 64),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(num_classes, activation='softmax')  # softmax вместо sigmoid\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # другая функция потерь\n",
    "              metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
